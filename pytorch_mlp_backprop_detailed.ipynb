{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# PyTorch Multi-Layer Perceptrons (MLPs) with Backpropagation: An In-Depth Exploration\n",
    "\n",
    "Welcome to this comprehensive tutorial on Multi-Layer Perceptrons (MLPs) and backpropagation using PyTorch! In this notebook, we will dive deep into the inner workings of MLPs, the backpropagation algorithm, and how PyTorch automates this process. We'll cover everything from the mathematics behind neural networks to the implementation details in PyTorch.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Multi-Layer Perceptrons](#introduction-to-multi-layer-perceptrons)\n",
    "2. [The Mathematics of Neural Networks](#the-mathematics-of-neural-networks)\n",
    "3. [Backpropagation: The Heart of Deep Learning](#backpropagation-the-heart-of-deep-learning)\n",
    "4. [Implementing an MLP from Scratch](#implementing-an-mlp-from-scratch)\n",
    "5. [PyTorch's Autograd: Automatic Differentiation](#pytorchs-autograd-automatic-differentiation)\n",
    "6. [Building and Training an MLP in PyTorch](#building-and-training-an-mlp-in-pytorch)\n",
    "7. [Visualizing Gradients and Training Dynamics](#visualizing-gradients-and-training-dynamics)\n",
    "8. [Advanced Topics and Best Practices](#advanced-topics-and-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction-to-mlp",
   "metadata": {},
   "source": [
    "## 1. Introduction to Multi-Layer Perceptrons\n",
    "\n",
    "Multi-Layer Perceptrons (MLPs) are the foundation of deep learning. They consist of multiple layers of interconnected nodes, or \"neurons\", that can learn complex patterns in data.\n",
    "\n",
    "### Structure of an MLP\n",
    "\n",
    "An MLP typically consists of:\n",
    "1. An input layer\n",
    "2. One or more hidden layers\n",
    "3. An output layer\n",
    "\n",
    "Each neuron in one layer is connected to every neuron in the next layer, forming a fully connected network.\n",
    "\n",
    "Let's visualize a simple MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlp-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mlp(input_size, hidden_sizes, output_size):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    left = 0.1\n",
    "    for i, size in enumerate(layer_sizes):\n",
    "        for j in range(size):\n",
    "            circle = plt.Circle((left, j/size), 0.02, fill=False)\n",
    "            ax.add_artist(circle)\n",
    "        if i < len(layer_sizes) - 1:\n",
    "            for j in range(size):\n",
    "                for k in range(layer_sizes[i+1]):\n",
    "                    ax.plot([left, left+0.3], [j/size, k/layer_sizes[i+1]], 'gray', alpha=0.2)\n",
    "        left += 0.3\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Multi-Layer Perceptron Architecture')\n",
    "    plt.show()\n",
    "\n",
    "plot_mlp(input_size=4, hidden_sizes=[5, 3], output_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-of-nn",
   "metadata": {},
   "source": [
    "## 2. The Mathematics of Neural Networks\n",
    "\n",
    "At its core, each neuron in an MLP performs a simple computation:\n",
    "\n",
    "1. It takes a weighted sum of its inputs.\n",
    "2. It adds a bias term.\n",
    "3. It applies an activation function to this sum.\n",
    "\n",
    "Mathematically, for a single neuron:\n",
    "\n",
    "$$y = f(\\sum_{i=1}^n w_i x_i + b)$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ are the inputs\n",
    "- $w_i$ are the weights\n",
    "- $b$ is the bias\n",
    "- $f$ is the activation function\n",
    "\n",
    "Let's implement a single neuron to understand this better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-neuron",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = torch.randn(input_size)\n",
    "        self.bias = torch.randn(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(torch.dot(self.weights, x) + self.bias)\n",
    "\n",
    "# Test the neuron\n",
    "neuron = Neuron(3)\n",
    "input_data = torch.tensor([1.0, 2.0, 3.0])\n",
    "output = neuron.forward(input_data)\n",
    "print(f\"Neuron output: {output.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backpropagation",
   "metadata": {},
   "source": [
    "## 3. Backpropagation: The Heart of Deep Learning\n",
    "\n",
    "Backpropagation is the algorithm that allows neural networks to learn. It's a way of computing gradients of the loss function with respect to the network's parameters (weights and biases).\n",
    "\n",
    "The key idea is to use the chain rule of calculus to efficiently compute these gradients. Let's break down the process:\n",
    "\n",
    "1. Forward pass: Compute the output of the network for a given input.\n",
    "2. Compute the loss: Compare the output to the true target.\n",
    "3. Backward pass: Compute the gradient of the loss with respect to each parameter, starting from the output layer and moving backwards.\n",
    "\n",
    "Let's implement a simple example of backpropagation for a single neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backprop-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class NeuronWithBackprop:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = torch.randn(input_size, requires_grad=True)\n",
    "        self.bias = torch.randn(1, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return sigmoid(torch.dot(self.weights, x) + self.bias)\n",
    "    \n",
    "    def backward(self, x, y, output):\n",
    "        # Compute gradients\n",
    "        d_loss = 2 * (output - y)  # Derivative of MSE loss\n",
    "        d_output = sigmoid_derivative(output)\n",
    "        d_weights = d_loss * d_output * x\n",
    "        d_bias = d_loss * d_output\n",
    "        \n",
    "        # Update weights and bias\n",
    "        learning_rate = 0.1\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "\n",
    "# Train the neuron\n",
    "neuron = NeuronWithBackprop(2)\n",
    "x = torch.tensor([0.5, 1.0])\n",
    "y = torch.tensor(0.7)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    output = neuron.forward(x)\n",
    "    loss = (output - y) ** 2\n",
    "    neuron.backward(x, y, output)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Output: {output.item():.4f}\")\n",
    "\n",
    "print(f\"Final output: {neuron.forward(x).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlp-from-scratch",
   "metadata": {},
   "source": [
    "## 4. Implementing an MLP from Scratch\n",
    "\n",
    "Now that we understand the basics of neurons and backpropagation, let's implement a full MLP from scratch. This will help us understand the inner workings of neural networks before we use PyTorch's built-in modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlp-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append({\n",
    "                'weights': torch.randn(layer_sizes[i+1], layer_sizes[i], requires_grad=True),\n",
    "                'bias': torch.randn(layer_sizes[i+1], 1, requires_grad=True)\n",
    "            })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for layer in self.layers:\n",
    "            z = torch.mm(layer['weights'], activations[-1]) + layer['bias']\n",
    "            a = torch.sigmoid(z)\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, x, y, learning_rate=0.1):\n",
    "        activations = self.forward(x)\n",
    "        n_layers = len(self.layers)\n",
    "        \n",
    "        # Compute output layer gradients\n",
    "        delta = activations[-1] - y\n",
    "        \n",
    "        for i in reversed(range(n_layers)):\n",
    "            layer = self.layers[i]\n",
    "            a = activations[i+1]\n",
    "            \n",
    "            # Compute gradients\n",
    "            d_weights = torch.mm(delta, activations[i].t())\n",
    "            d_bias = delta.sum(1, keepdim=True)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            layer['weights'] -= learning_rate * d_weights\n",
    "            layer['bias'] -= learning_rate * d_bias\n",
    "            \n",
    "            # Compute delta for next layer\n",
    "            if i > 0:\n",
    "                delta = torch.mm(layer['weights'].t(), delta) * (a * (1 - a))\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(X, y)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = torch.mean((self.forward(X)[-1] - y) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Create and train the MLP\n",
    "mlp = MLP([2, 3, 1])\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32).t()\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "mlp.train(X, y)\n",
    "\n",
    "# Test the trained MLP\n",
    "print(\"\\nFinal predictions:\")\n",
    "predictions = mlp.forward(X)[-1]\n",
    "for input, target, pred in zip(X.t(), y, predictions.t()):\n",
    "    print(f\"Input: {input.tolist()}, Target: {target.item():.0f}, Prediction: {pred.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "autograd",
   "metadata": {},
   "source": [
    "## 5. PyTorch's Autograd: Automatic Differentiation\n",
    "\n",
    "PyTorch's autograd package provides automatic differentiation for all operations on Tensors. It allows us to compute gradients automatically, without having to implement backpropagation manually.\n",
    "\n",
    "Let's see how autograd works with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autograd-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors with requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "z = x * y\n",
    "out = z.mean()\n",
    "\n",
    "# Compute gradients\n",
    "out.backward()\n",
    "\n",
    "print(\"Gradient of x:\", x.grad)\n",
    "print(\"Gradient of y:\", y.grad)\n",
    "\n",
    "# Let's visualize the computation graph\n",
    "from torchviz import make_dot\n",
    "make_dot(out, params={'x': x, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch-mlp",
   "metadata": {},
   "source": [
    "## 6. Building and Training an MLP in PyTorch\n",
    "\n",
    "Now that we understand the inner workings of MLPs and backpropagation, let's see how PyTorch simplifies the process of building and training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch-mlp-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create the MLP\n",
    "mlp = PyTorchMLP(input_size=2, hidden_sizes=[4, 4], output_size=1)\n",
    "print(mlp)\n",
    "\n",
    "# Prepare data\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = mlp(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = mlp(X)\n",
    "    for input, target, pred in zip(X, y, predictions):\n",
    "        print(f\"Input: {input.tolist()}, Target: {target.item():.0f}, Prediction: {pred.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualizing-gradients",
   "metadata": {},
   "source": [
    "## 7. Visualizing Gradients and Training Dynamics\n",
    "\n",
    "Understanding how gradients flow through the network and how they change during training can provide valuable insights. Let's visualize these aspects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients(model):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.hist(param.grad.numpy().flatten(), bins=50)\n",
    "            plt.title('Gradient Distribution')\n",
    "            plt.xlabel('Gradient Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.imshow(param.grad.numpy(), cmap='viridis')\n",
    "            plt.title(f'Gradient Heatmap - {name}')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.hist(param.data.numpy().flatten(), bins=50)\n",
    "            plt.title('Parameter Distribution')\n",
    "            plt.xlabel('Parameter Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.imshow(param.data.numpy(), cmap='viridis')\n",
    "            plt.title(f'Parameter Heatmap - {name}')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            break  # Just plot for one layer\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train the model and visualize gradients\n",
    "mlp = PyTorchMLP(input_size=2, hidden_sizes=[4, 4], output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = mlp(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        plot_gradients(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-topics",
   "metadata": {},
   "source": [
    "## 8. Advanced Topics and Best Practices\n",
    "\n",
    "1. **Initialization Techniques**: Proper weight initialization is crucial for training deep networks.\n",
    "2. **Regularization**: L1/L2 regularization, dropout, and batch normalization can help prevent overfitting.\n",
    "3. **Learning Rate Scheduling**: Adjusting the learning rate during training can lead to better convergence.\n",
    "4. **Gradient Clipping**: This technique can help prevent exploding gradients.\n",
    "5. **Advanced Optimizers**: Techniques like Adam, RMSprop, or SGD with momentum can improve training.\n",
    "\n",
    "Let's implement some of these advanced techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-techniques",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.5):\n",
    "        super(AdvancedMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "                self.layers.append(nn.Dropout(dropout_rate))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "advanced_mlp = AdvancedMLP(input_size=2, hidden_sizes=[32, 16], output_size=1)\n",
    "\n",
    "# Initialize weights using Xavier initialization\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "advanced_mlp.apply(init_weights)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(advanced_mlp.parameters(), lr=0.01, weight_decay=1e-5)  # L2 regularization\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    advanced_mlp.train()\n",
    "    outputs = advanced_mlp(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(advanced_mlp.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "advanced_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = advanced_mlp(X)\n",
    "    for input, target, pred in zip(X, y, predictions):\n",
    "        print(f\"Input: {input.tolist()}, Target: {target.item():.0f}, Prediction: {pred.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've taken a deep dive into the world of Multi-Layer Perceptrons and backpropagation. We've covered everything from the basic mathematics behind neural networks to advanced PyTorch implementations with various optimization techniques.\n",
    "\n",
    "Key takeaways:\n",
    "1. Understanding the mathematics behind neural networks is crucial for effective implementation and debugging.\n",
    "2. PyTorch's autograd system greatly simplifies the process of computing gradients.\n",
    "3. Advanced techniques like proper initialization, regularization, and learning rate scheduling can significantly improve model performance.\n",
    "4. Visualizing gradients and training dynamics can provide valuable insights into the learning process.\n",
    "\n",
    "As you continue your journey in deep learning, remember that these fundamental concepts form the backbone of more complex architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).\n",
    "\n",
    "Happy learning and experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
