{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa Tokenizer Implementation and Demonstration (From Scratch)\n",
    "\n",
    "This notebook contains a complete implementation of a DeBERTa tokenizer from scratch using PyTorch, along with demonstrations of its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict, Union, Optional\n",
    "\n",
    "class DeBERTaTokenizer:\n",
    "    def __init__(self, vocab_file: Optional[str] = None, max_len: int = 512, do_lower_case: bool = True):\n",
    "        self.max_len = max_len\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.vocab = OrderedDict()\n",
    "        self.ids_to_tokens = {}\n",
    "        self.special_tokens = {\n",
    "            '[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4\n",
    "        }\n",
    "        if vocab_file:\n",
    "            self.load_vocab(vocab_file)\n",
    "        else:\n",
    "            self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Compile regex pattern for tokenization\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def load_vocab(self, vocab_file: str):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                token = line.strip()\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "        self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "        tokens = []\n",
    "        for token in self.pat.findall(text):\n",
    "            if token in self.vocab:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                for char in token:\n",
    "                    if char in self.vocab:\n",
    "                        tokens.append(char)\n",
    "                    else:\n",
    "                        tokens.append('[UNK]')\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(token, self.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
    "        return [self.ids_to_tokens.get(id, '[UNK]') for id in ids]\n",
    "\n",
    "    def encode(self, text: Union[str, List[str]], text_pair: Optional[Union[str, List[str]]] = None, \n",
    "               max_length: Optional[int] = None, padding: bool = True, truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        if isinstance(text, str):\n",
    "            return self._encode_single(text, text_pair, max_length, padding, truncation)\n",
    "        \n",
    "        batch_encoding = {'input_ids': [], 'attention_mask': []}\n",
    "        for i, t in enumerate(text):\n",
    "            pair = text_pair[i] if text_pair is not None else None\n",
    "            encoding = self._encode_single(t, pair, max_length, padding, truncation)\n",
    "            for key, value in encoding.items():\n",
    "                batch_encoding[key].append(value)\n",
    "        \n",
    "        return {k: torch.stack(v) for k, v in batch_encoding.items()}\n",
    "\n",
    "    def _encode_single(self, text: str, text_pair: Optional[str] = None, \n",
    "                       max_length: Optional[int] = None, padding: bool = True, \n",
    "                       truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        tokens = ['[CLS]'] + self.tokenize(text) + ['[SEP]']\n",
    "        if text_pair:\n",
    "            tokens += self.tokenize(text_pair) + ['[SEP]']\n",
    "        \n",
    "        if truncation and len(tokens) > (max_length or self.max_len):\n",
    "            tokens = tokens[:(max_length or self.max_len) - 1] + ['[SEP]']\n",
    "        \n",
    "        ids = self.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if padding and len(ids) < (max_length or self.max_len):\n",
    "            ids += [self.vocab['[PAD]']] * ((max_length or self.max_len) - len(ids))\n",
    "        \n",
    "        attention_mask = [1 if id != self.vocab['[PAD]'] else 0 for id in ids]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids),\n",
    "            'attention_mask': torch.tensor(attention_mask)\n",
    "        }\n",
    "\n",
    "    def decode(self, ids: Union[List[int], torch.Tensor], skip_special_tokens: bool = True) -> str:\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "        tokens = self.convert_ids_to_tokens(ids)\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in self.special_tokens.keys()]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def add_tokens(self, new_tokens: List[str]) -> int:\n",
    "        added = 0\n",
    "        for token in new_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.ids_to_tokens[len(self.vocab) - 1] = token\n",
    "                added += 1\n",
    "        return added\n",
    "\n",
    "# Helper function to create a small vocabulary for demonstration\n",
    "def create_small_vocab():\n",
    "    vocab = OrderedDict({\n",
    "        '[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4,\n",
    "        'hello': 5, 'world': 6, 'how': 7, 'are': 8, 'you': 9,\n",
    "        'this': 10, 'is': 11, 'a': 12, 'test': 13\n",
    "    })\n",
    "    return vocab\n",
    "\n",
    "# Initialize tokenizer with a small vocabulary\n",
    "small_vocab = create_small_vocab()\n",
    "tokenizer = DeBERTaTokenizer()\n",
    "tokenizer.vocab = small_vocab\n",
    "tokenizer.ids_to_tokens = {v: k for k, v in small_vocab.items()}\n",
    "\n",
    "print(\"DeBERTa Tokenizer implemented from scratch and initialized with a small vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello world how are you\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding Single Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello world\"\n",
    "encoding = tokenizer.encode(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Encoded input_ids: {encoding['input_ids']}\")\n",
    "print(f\"Attention mask: {encoding['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Text Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = \"hello world\"\n",
    "text_b = \"how are you\"\n",
    "encoding = tokenizer.encode(text_a, text_b)\n",
    "print(f\"Text A: {text_a}\")\n",
    "print(f\"Text B: {text_b}\")\n",
    "print(f\"Encoded input_ids: {encoding['input_ids']}\")\n",
    "print(f\"Attention mask: {encoding['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello world\", \"how are you\"]\n",
    "encodings = tokenizer.encode(texts)\n",
    "print(f\"Batch texts: {texts}\")\n",
    "print(f\"Encoded input_ids: {encodings['input_ids']}\")\n",
    "print(f\"Attention masks: {encodings['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [2, 5, 6, 3, 0, 0]  # [CLS] hello world [SEP] [PAD] [PAD]\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"Original ids: {ids}\")\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Unknown Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello unknown world\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding New Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = ['new', 'tokens']\n",
    "num_added = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Number of tokens added: {num_added}\")\n",
    "\n",
    "text = \"hello new tokens world\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokenized with new tokens: {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
