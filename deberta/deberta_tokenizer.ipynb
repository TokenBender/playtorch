{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa Tokenizer Implementation and Demonstration (From Scratch)\n",
    "\n",
    "This notebook contains a complete implementation of a DeBERTa tokenizer from scratch using PyTorch, along with demonstrations of its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 175\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Test the tokenizer\u001b[39;00m\n\u001b[1;32m    174\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world! This is a test.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 175\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(test_text)\n",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m, in \u001b[0;36mDeBERTaTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     91\u001b[0m     subword \u001b[38;5;241m=\u001b[39m word\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subword) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m subword\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m     94\u001b[0m             tokens\u001b[38;5;241m.\u001b[39mappend(subword\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</w>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import regex as re\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "\n",
    "class DeBERTaTokenizer:\n",
    "    def __init__(self, vocab_file: Optional[str] = None, vocab_size: int = 30000, min_frequency: int = 2, max_len: int = 512, do_lower_case: bool = True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.max_len = max_len\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.vocab = OrderedDict()\n",
    "        self.ids_to_tokens = {}\n",
    "        self.merges = {}\n",
    "        self.special_tokens = {\n",
    "            '[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4\n",
    "        }\n",
    "        if vocab_file:\n",
    "            self.load_vocab(vocab_file)\n",
    "        else:\n",
    "            self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Compile regex pattern for tokenization\n",
    "        self.pat = re.compile(r\"\"\"(?:https?:\\/\\/)?(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)|[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|'s|'t|'re|'ve|'m|'ll|'d|#\\w+|@\\w+| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def load_vocab(self, vocab_file: str):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                token = line.strip()\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "        self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        # Count initial characters\n",
    "        char_counts = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for char in text:\n",
    "                char_counts[char] += 1\n",
    "\n",
    "        # Initialize vocabulary with characters\n",
    "        self.vocab = OrderedDict({char: i for i, (char, count) in enumerate(char_counts.items()) if count >= self.min_frequency})\n",
    "        self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "        # Tokenize texts into characters\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = self.pat.findall(text)\n",
    "            for word in words:\n",
    "                word_counts[' '.join(list(word)) + ' </w>'] += 1\n",
    "\n",
    "        # Iteratively merge most frequent pair\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = self.get_stats(word_counts)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.merge_vocab(best, word_counts)\n",
    "            self.merges[best] = len(self.vocab)\n",
    "            new_token = ''.join(best)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.ids_to_tokens[len(self.vocab) - 1] = new_token\n",
    "\n",
    "    def get_stats(self, word_counts: Counter) -> Dict[Tuple[str, str], int]:\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in word_counts.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair: Tuple[str, str], word_counts: Counter):\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        replacement = ''.join(pair)\n",
    "        for word in list(word_counts.keys()):\n",
    "            new_word = pattern.sub(replacement, word)\n",
    "            if new_word != word:\n",
    "                word_counts[new_word] = word_counts.pop(word)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "        words = self.pat.findall(text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            word = ' '.join(list(word)) + ' </w>'\n",
    "            while len(word) > 0:\n",
    "                subword = word\n",
    "                while len(subword) > 0:\n",
    "                    if subword.strip() in self.vocab:\n",
    "                        tokens.append(subword.strip().replace('</w>', ''))\n",
    "                        word = word[len(subword):]\n",
    "                        break\n",
    "                    subword = subword.rsplit(' ', 1)[0]\n",
    "                if len(subword) == 0:\n",
    "                    tokens.append('[UNK]')\n",
    "                    word = word[1:]\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(token, self.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
    "        return [self.ids_to_tokens.get(id, '[UNK]') for id in ids]\n",
    "\n",
    "    def encode(self, text: Union[str, List[str]], text_pair: Optional[Union[str, List[str]]] = None, \n",
    "               max_length: Optional[int] = None, padding: bool = True, truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        if isinstance(text, str):\n",
    "            return self._encode_single(text, text_pair, max_length, padding, truncation)\n",
    "        \n",
    "        batch_encoding = {'input_ids': [], 'attention_mask': []}\n",
    "        for i, t in enumerate(text):\n",
    "            pair = text_pair[i] if text_pair is not None else None\n",
    "            encoding = self._encode_single(t, pair, max_length, padding, truncation)\n",
    "            for key, value in encoding.items():\n",
    "                batch_encoding[key].append(value)\n",
    "        \n",
    "        return {k: torch.stack(v) for k, v in batch_encoding.items()}\n",
    "\n",
    "    def _encode_single(self, text: str, text_pair: Optional[str] = None, \n",
    "                       max_length: Optional[int] = None, padding: bool = True, \n",
    "                       truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        tokens = ['[CLS]'] + self.tokenize(text) + ['[SEP]']\n",
    "        if text_pair:\n",
    "            tokens += self.tokenize(text_pair) + ['[SEP]']\n",
    "        \n",
    "        if truncation and len(tokens) > (max_length or self.max_len):\n",
    "            tokens = tokens[:(max_length or self.max_len) - 1] + ['[SEP]']\n",
    "        \n",
    "        ids = self.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if padding and len(ids) < (max_length or self.max_len):\n",
    "            ids += [self.vocab['[PAD]']] * ((max_length or self.max_len) - len(ids))\n",
    "        \n",
    "        attention_mask = [1 if id != self.vocab['[PAD]'] else 0 for id in ids]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids),\n",
    "            'attention_mask': torch.tensor(attention_mask)\n",
    "        }\n",
    "\n",
    "    def decode(self, ids: Union[List[int], torch.Tensor], skip_special_tokens: bool = True) -> str:\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "        tokens = self.convert_ids_to_tokens(ids)\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in self.special_tokens.keys()]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def add_tokens(self, new_tokens: List[str]) -> int:\n",
    "        added = 0\n",
    "        for token in new_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.ids_to_tokens[len(self.vocab) - 1] = token\n",
    "                added += 1\n",
    "        return added\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"Hello world! How are you doing today?\",\n",
    "    \"This is an example of tokenizer training.\",\n",
    "    \"We need many diverse sentences to train effectively.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "tokenizer = DeBERTaTokenizer(vocab_size=100)  # Small vocab_size for demonstration\n",
    "tokenizer.train(texts)\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Hello world! This is a test.\"\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"Tokenized: {tokens}\")\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: hello world how are you\n",
      "Tokenized: ['hello', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "text = \"hello world how are you\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding Single Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: hello world\n",
      "Encoded input_ids: tensor([2, 5, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "text = \"hello world\"\n",
    "encoding = tokenizer.encode(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Encoded input_ids: {encoding['input_ids']}\")\n",
    "print(f\"Attention mask: {encoding['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Text Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = \"hello world\"\n",
    "text_b = \"how are you\"\n",
    "encoding = tokenizer.encode(text_a, text_b)\n",
    "print(f\"Text A: {text_a}\")\n",
    "print(f\"Text B: {text_b}\")\n",
    "print(f\"Encoded input_ids: {encoding['input_ids']}\")\n",
    "print(f\"Attention mask: {encoding['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"hello world\", \"how are you\"]\n",
    "encodings = tokenizer.encode(texts)\n",
    "print(f\"Batch texts: {texts}\")\n",
    "print(f\"Encoded input_ids: {encodings['input_ids']}\")\n",
    "print(f\"Attention masks: {encodings['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [2, 5, 6, 3, 0, 0]  # [CLS] hello world [SEP] [PAD] [PAD]\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"Original ids: {ids}\")\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Unknown Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello unknown world\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding New Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = ['new', 'tokens']\n",
    "num_added = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Number of tokens added: {num_added}\")\n",
    "\n",
    "text = \"hello new tokens world\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokenized with new tokens: {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
