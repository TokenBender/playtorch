<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PyTorch Basics and Multi-Layer Perceptrons (MLPs) Explained</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        pre {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 10px;
            margin: 10px 0;
            overflow-x: auto;
        }
        code {
            font-family: monospace;
        }
        .note {
            background-color: #e7f3fe;
            border-left: 6px solid #2196F3;
            margin-bottom: 15px;
            padding: 4px 12px;
        }
        .warning {
            background-color: #ffffcc;
            border-left: 6px solid #ffeb3b;
            margin-bottom: 15px;
            padding: 4px 12px;
        }
    </style>
</head>
<body>
    <h1>PyTorch Basics and Multi-Layer Perceptrons (MLPs) Explained</h1>
    
    <h2>1. Introduction to PyTorch and Tensors</h2>
    <p>PyTorch is a powerful library for machine learning, especially deep learning. It's like a toolbox full of useful tools for building and training neural networks.</p>
    <p>The most basic building block in PyTorch is called a <strong>tensor</strong>. You can think of a tensor as a container for numbers, similar to a list or an array in other programming languages. The cool thing about tensors is that they can have multiple dimensions, making them perfect for representing complex data like images or text.</p>
    
    <h3>Creating Tensors</h3>
    <pre><code>import torch

# Create a 1D tensor (vector)
vector = torch.tensor([1, 2, 3, 4, 5])

# Create a 2D tensor (matrix)
matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Create a 3D tensor
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])</code></pre>
    
    <p>In this example, we create tensors of different dimensions. The 1D tensor is like a list, the 2D tensor is like a table or matrix, and the 3D tensor is like a stack of 2D matrices.</p>
    
    <div class="note">
        <p><strong>Intuition:</strong> Think of tensors as flexible containers that can hold numbers in various shapes. A 1D tensor is like a line of numbers, a 2D tensor is like a grid of numbers, and a 3D tensor is like a cube of numbers.</p>
    </div>
    
    <div class="warning">
        <p><strong>Common Mistake:</strong> Beginners often forget that PyTorch uses zero-based indexing. This means the first element of a tensor is accessed with index 0, not 1.</p>
    </div>
    
    <h2>2. Tensor Operations and Matrix Math</h2>
    <p>Tensors aren't just for storing numbers; we can perform various operations on them. These operations are the building blocks of neural network computations.</p>
    
    <h3>Basic Operations</h3>
    <pre><code># Addition
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
c = a + b  # Result: tensor([5, 7, 9])

# Multiplication
d = a * b  # Element-wise multiplication, result: tensor([4, 10, 18])

# Matrix multiplication
m1 = torch.tensor([[1, 2], [3, 4]])
m2 = torch.tensor([[5, 6], [7, 8]])
m3 = torch.matmul(m1, m2)  # Result: tensor([[19, 22], [43, 50]])</code></pre>
    
    <p>These operations allow us to manipulate tensors in various ways, which is crucial for building and training neural networks.</p>
    
    <div class="note">
        <p><strong>Intuition:</strong> Think of tensor operations as ways to combine or transform the numbers inside tensors. Addition is like combining two sets of numbers, while matrix multiplication is a more complex way of combining numbers that's especially useful in neural networks.</p>
    </div>
    
    <div class="warning">
        <p><strong>Common Mistake:</strong> Beginners often confuse element-wise multiplication (*) with matrix multiplication (torch.matmul or @). Make sure you use the right operation for your task!</p>
    </div>
    
    <h2>3. Building Blocks of Neural Networks</h2>
    <p>Neural networks are made up of several key components. Let's explore each one:</p>
    
    <h3>Linear Layers</h3>
    <p>A linear layer performs a linear transformation on the input data. It's like applying a mathematical function y = mx + b to each input.</p>
    <pre><code>import torch.nn as nn

linear = nn.Linear(in_features=10, out_features=5)</code></pre>
    <p>This creates a linear layer that takes 10 input features and produces 5 output features.</p>
    
    <h3>Activation Functions</h3>
    <p>Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.</p>
    <pre><code>relu = nn.ReLU()
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()</code></pre>
    <p>These are different types of activation functions, each with its own characteristics:</p>
    <ul>
        <li>ReLU (Rectified Linear Unit): Returns 0 for negative inputs, and the input itself for positive inputs.</li>
        <li>Sigmoid: Squashes inputs to a range between 0 and 1.</li>
        <li>Tanh: Squashes inputs to a range between -1 and 1.</li>
    </ul>
    
    <div class="note">
        <p><strong>Intuition:</strong> Think of activation functions as decision-makers. They look at the input and decide how much of that information should be passed on to the next layer.</p>
    </div>
    
    <h3>Dropout</h3>
    <p>Dropout is a regularization technique that helps prevent overfitting by randomly "dropping out" (setting to zero) a proportion of neurons during training.</p>
    <pre><code>dropout = nn.Dropout(p=0.5)</code></pre>
    <p>This creates a dropout layer that will randomly zero out 50% of the inputs during training.</p>
    
    <div class="warning">
        <p><strong>Common Question:</strong> "Why do we need dropout?" Dropout helps the network learn more robust features by preventing it from relying too heavily on any particular neuron.</p>
    </div>
    
    <h3>Batch Normalization</h3>
    <p>Batch normalization normalizes the inputs to a layer for each mini-batch, which can help the network train faster and more stably.</p>
    <pre><code>batch_norm = nn.BatchNorm1d(10)</code></pre>
    <p>This creates a batch normalization layer for inputs with 10 features.</p>
    
    <h2>4. Creating a Multi-Layer Perceptron (MLP)</h2>
    <p>Now that we understand the building blocks, let's put them together to create an MLP!</p>
    <pre><code>class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MLP, self).__init__()
        self.layers = nn.ModuleList()
        
        # Input layer
        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))
        self.layers.append(nn.ReLU())
        
        # Hidden layers
        for i in range(len(hidden_sizes) - 1):
            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))
            self.layers.append(nn.ReLU())
        
        # Output layer
        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# Create an instance of the MLP
mlp = MLP(input_size=10, hidden_sizes=[64, 32], output_size=1)</code></pre>
    
    <p>This code defines an MLP with an input size of 10, two hidden layers with 64 and 32 neurons respectively, and an output size of 1.</p>
    
    <div class="note">
        <p><strong>Intuition:</strong> Think of an MLP as a series of transformations. Each layer takes some input, transforms it, and passes it to the next layer. The final layer produces the output.</p>
    </div>
    
    <div class="warning">
        <p><strong>Common Mistake:</strong> Beginners often forget to apply activation functions between linear layers. Without these non-linearities, the entire network would just be one big linear transformation!</p>
    </div>
    
    <h2>5. Training an MLP</h2>
    <p>Training an MLP involves feeding it data, comparing its predictions to the true values, and adjusting its parameters to improve its predictions. This process is called backpropagation.</p>
    
    <h3>Key Components of Training</h3>
    <ul>
        <li><strong>Loss Function:</strong> Measures how far off the model's predictions are from the true values.</li>
        <li><strong>Optimizer:</strong> Adjusts the model's parameters to minimize the loss.</li>
        <li><strong>Training Loop:</strong> Repeatedly feeds data through the model, computes the loss, and updates the parameters.</li>
    </ul>
    
    <pre><code>import torch.optim as optim

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(mlp.parameters(), lr=0.001)

# Training loop
for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        # Forward pass
        outputs = mlp(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
    
    <div class="note">
        <p><strong>Intuition:</strong> Think of training as teaching the network. You show it examples (data), see how well it does (loss), and then give it feedback on how to improve (backpropagation).</p>
    </div>
    
    <div class="warning">
        <p><strong>Common Mistake:</strong> Forgetting to zero out the gradients (optimizer.zero_grad()) before the backward pass. This can lead to incorrect gradient calculations and poor training.</p>
    </div>
    
    <h2>6. Evaluating and Using the Trained MLP</h2>
    <p>After training, we need to evaluate how well our model performs on new, unseen data.</p>
    
    <pre><code># Evaluation
mlp.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient calculation
    test_outputs = mlp(X_test)
    test_loss = criterion(test_outputs, y_test)
    # For binary classification, we consider outputs > 0.5 as positive predictions
    predicted_labels = (test_outputs > 0.5).float()
    accuracy = (predicted_labels == y_test).float().mean()

print(f"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}")

# Make predictions on new data
new_data = torch.randn(5, input_size)  # Create 5 random input samples
predictions = mlp(new_data)  # Get model predictions for the new data
print("Predictions for new data:", predictions)</code></pre>
    
    <div class="note">
        <p><strong>Intuition:</strong> Evaluation is like giving your trained model a final exam. You test it on data it hasn't seen before to see how well it has learned to generalize.</p>
    </div>
    
    <div class="warning">
        <p><strong>Common Mistake:</strong> Forgetting to set the model to evaluation mode (model.eval()). This can lead to incorrect results, especially if you're using layers like Dropout or BatchNorm.</p>
    </div>
    
    <h2>Conclusion</h2>
    <p>We've covered the basics of PyTorch and Multi-Layer Perceptrons, from tensors and basic operations to building, training, and evaluating a neural network. Remember, practice is key to mastering these concepts. Don't be afraid to experiment and make mistakes - that's how you learn!</p>
    
    <p>Happy coding and machine learning!</p>
</body>
</html>