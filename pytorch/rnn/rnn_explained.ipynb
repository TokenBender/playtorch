{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) Explained: A Comprehensive Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to RNNs](#introduction)\n",
    "2. [The Problem: Understanding Sequential Data](#problem)\n",
    "3. [The Intuition: How Humans Process Sequences](#intuition)\n",
    "4. [Basic Structure of an RNN](#structure)\n",
    "5. [Types of RNNs and Their Applications](#types)\n",
    "6. [The Math Behind RNNs](#math)\n",
    "7. [Training RNNs: Backpropagation Through Time](#training)\n",
    "8. [The Vanishing Gradient Problem](#vanishing)\n",
    "9. [Long Short-Term Memory (LSTM) Networks](#lstm)\n",
    "10. [Gated Recurrent Units (GRUs)](#gru)\n",
    "11. [Practical Considerations and Best Practices](#practical)\n",
    "12. [Implementing RNNs with PyTorch: A Case Study](#implementing)\n",
    "13. [Common Applications and Real-world Examples](#applications)\n",
    "14. [Limitations of RNNs and Future Directions](#limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to RNNs <a name=\"introduction\"></a>\n",
    "\n",
    "Imagine you're reading a book. As you read each word, your understanding of the story doesn't start from scratch - it builds upon what you've read before. This is exactly what Recurrent Neural Networks (RNNs) do with sequential data. They're a class of neural networks designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or numerical time series data.\n",
    "\n",
    "**Key Point:** RNNs are neural networks with loops, allowing information to persist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem: Understanding Sequential Data <a name=\"problem\"></a>\n",
    "\n",
    "Traditional neural networks fall short when it comes to sequential data. They assume that all inputs (and outputs) are independent of each other. But for many tasks, that's not the case. If you want to predict the next word in a sentence, you need to know the words that came before it.\n",
    "\n",
    "**Example:** In the sentence \"The clouds are in the ___\", you'd probably guess \"sky\". But in \"The kids are in the ___\", you might guess \"playground\" or \"school\". The context matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Intuition: How Humans Process Sequences <a name=\"intuition\"></a>\n",
    "\n",
    "Think about how you understand language. When you read a sentence, you don't start from scratch with each word. You understand each word based on your understanding of the previous words. RNNs work similarly.\n",
    "\n",
    "**Example:** Consider the sentence: \"The cat sat on the ___.\"\n",
    "Your brain automatically fills in \"mat\" or \"chair\" because it has learned patterns from previous experiences. An RNN does the same thing by maintaining a \"memory\" of previous inputs.\n",
    "\n",
    "**Analogy:** An RNN is like a chain of repeated neural networks, each passing a message to its successor. Imagine a game of telephone, but instead of distorting the message, each person adds relevant information before passing it on.\n",
    "\n",
    "Let's break this down further:\n",
    "1. **Sequential Processing:** Just as you read a sentence word by word, an RNN processes data step by step.\n",
    "2. **Memory:** Your brain retains important information from previous words. Similarly, an RNN has a \"hidden state\" that acts as its memory.\n",
    "3. **Context:** You interpret each word based on the context of previous words. An RNN uses its hidden state to provide context for processing each new input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Structure of an RNN <a name=\"structure\"></a>\n",
    "\n",
    "An RNN has a 'memory' which captures information about what has been calculated so far. Let's break down its structure:\n",
    "\n",
    "- **Input (x_t):** The input at the current time step.\n",
    "- **Hidden State (h_t):** The 'memory' of the network.\n",
    "- **Output (y_t):** The output at the current time step.\n",
    "\n",
    "The basic equations of an RNN are:\n",
    "\n",
    "```\n",
    "h_t = tanh(W_hh * h_(t-1) + W_xh * x_t)\n",
    "y_t = W_hy * h_t\n",
    "```\n",
    "\n",
    "Where W_hh, W_xh, and W_hy are weight matrices that are learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Types of RNNs and Their Applications <a name=\"types\"></a>\n",
    "\n",
    "RNNs come in various flavors, each suited for different tasks:\n",
    "\n",
    "- **One-to-One:** Standard neural network\n",
    "- **One-to-Many:** Image captioning (image → sequence of words)\n",
    "- **Many-to-One:** Sentiment classification (sequence of words → sentiment)\n",
    "- **Many-to-Many (Synchronized):** Video classification on a frame level\n",
    "- **Many-to-Many (Sequence-to-Sequence):** Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Math Behind RNNs <a name=\"math\"></a>\n",
    "\n",
    "At each time step t, an RNN performs the following computations:\n",
    "\n",
    "```\n",
    "h_t = tanh(W_hh * h_(t-1) + W_xh * x_t + b_h)\n",
    "y_t = W_hy * h_t + b_y\n",
    "```\n",
    "\n",
    "Where:\n",
    "- h_t is the hidden state at time t\n",
    "- x_t is the input at time t\n",
    "- y_t is the output at time t\n",
    "- W_hh, W_xh, W_hy are weight matrices\n",
    "- b_h and b_y are bias vectors\n",
    "- tanh is the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training RNNs: Backpropagation Through Time <a name=\"training\"></a>\n",
    "\n",
    "RNNs are trained using Backpropagation Through Time (BPTT). It's similar to regular backpropagation, but we sum up the gradients for each parameter across all time steps.\n",
    "\n",
    "**Challenge:** As the sequence gets longer, gradients can either vanish or explode, making it hard to capture long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Vanishing Gradient Problem <a name=\"vanishing\"></a>\n",
    "\n",
    "In long sequences, information from the early steps tends to get lost as it's repeatedly multiplied by small numbers (weights) during backpropagation. This is known as the vanishing gradient problem.\n",
    "\n",
    "**Solution Preview:** LSTMs and GRUs were designed to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Long Short-Term Memory (LSTM) Networks <a name=\"lstm\"></a>\n",
    "\n",
    "LSTMs are a special kind of RNN capable of learning long-term dependencies. They have a more complex structure with gates that regulate the flow of information:\n",
    "\n",
    "- **Forget Gate:** Decides what information to throw away from the cell state.\n",
    "- **Input Gate:** Decides which values we'll update.\n",
    "- **Output Gate:** Decides what parts of the cell state we're going to output.\n",
    "\n",
    "Here's a simple implementation of an LSTM in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gated Recurrent Units (GRUs) <a name=\"gru\"></a>\n",
    "\n",
    "GRUs are a simpler variation of LSTMs. They combine the forget and input gates into a single \"update gate\" and merge the cell state and hidden state.\n",
    "\n",
    "**Tip:** GRUs are computationally more efficient than LSTMs and often perform just as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Practical Considerations and Best Practices <a name=\"practical\"></a>\n",
    "\n",
    "- **Gradient Clipping:** To prevent exploding gradients, clip them to a maximum value.\n",
    "- **Proper Initialization:** Initialize weights carefully to help with training stability.\n",
    "- **Bidirectional RNNs:** Process sequences both forward and backward for better context understanding.\n",
    "- **Attention Mechanisms:** Allow the model to focus on different parts of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Implementing RNNs with PyTorch: A Case Study <a name=\"implementing\"></a>\n",
    "\n",
    "Let's walk through a practical example of using RNNs to solve a real problem: predicting the next character in a sequence. This is a fundamental task in natural language processing and can be extended to more complex applications like text generation.\n",
    "\n",
    "### Problem: Character-level Language Model\n",
    "\n",
    "We'll create a model that, given a sequence of characters, predicts the next character. This can be used to generate text one character at a time.\n",
    "\n",
    "### Step 1: Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "\n",
    "# Sample text (you can use a larger corpus for better results)\n",
    "text = \"Hello world! How are you doing today? I hope you're having a great day!\"\n",
    "\n",
    "# Create character to index and index to character mappings\n",
    "chars = string.printable\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Convert text to indices\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "print(f\"Sample data: {data[:20]}\")\n",
    "print(f\"Decoded sample: {''.join([idx_to_char[idx] for idx in data[:20]])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Creating Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, seq_length):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.text[idx:idx+self.seq_length]),\n",
    "            torch.tensor(self.text[idx+1:idx+self.seq_length+1])\n",
    "        )\n",
    "\n",
    "# Create dataset and dataloader\n",
    "seq_length = 50\n",
    "dataset = TextDataset(data, seq_length)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check a sample from the dataloader\n",
    "inputs, targets = next(iter(dataloader))\n",
    "print(f\"Input shape: {inputs.shape}, Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Defining the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "# Instantiate the model\n",
    "n_characters = len(chars)\n",
    "hidden_size = 128\n",
    "model = CharRNN(n_characters, hidden_size, n_characters)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    hidden = model.init_hidden(32)\n",
    "    for inputs, targets in dataloader:\n",
    "        hidden = hidden.detach()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs.transpose(1, 2), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, length=100):\n",
    "    model.eval()\n",
    "    chars = [char_to_idx[ch] for ch in start_string]\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.tensor([chars[-1]]).unsqueeze(0)\n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            # Sample from the network as a multinomial distribution\n",
    "            probs = nn.functional.softmax(output[:, -1], dim=1)\n",
    "            pred = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            chars.append(pred)\n",
    "    \n",
    "    return ''.join([idx_to_char[idx] for idx in chars])\n",
    "\n",
    "# Generate some text\n",
    "print(generate_text(model, \"Hello\", length=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case study demonstrates how RNNs can be used for sequence modeling tasks. The same principles can be applied to various problems involving sequential data, such as time series prediction, sentiment analysis, or machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Common Applications and Real-world Examples <a name=\"applications\"></a>\n",
    "\n",
    "- **Natural Language Processing:** Language modeling, machine translation, sentiment analysis\n",
    "- **Speech Recognition:** Converting spoken language to text\n",
    "- **Time Series Prediction:** Stock prices, weather forecasting\n",
    "- **Music Generation:** Creating new melodies based on learned patterns\n",
    "- **Video Analysis:** Action recognition in videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Limitations of RNNs and Future Directions <a name=\"limitations\"></a>\n",
    "\n",
    "While powerful, RNNs (including LSTMs and GRUs) have limitations:\n",
    "- Difficulty in capturing very long-term dependencies\n",
    "- Computational inefficiency for very long sequences\n",
    "- Lack of parallelization in training\n",
    "\n",
    "**Future Directions:** Transformer models have largely superseded RNNs in many NLP tasks due to their ability to parallelize and capture long-range dependencies more effectively. However, RNNs still have their place, especially in scenarios where sequential processing is crucial or when working with limited computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
