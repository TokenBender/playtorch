{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics and Multi-Layer Perceptrons (MLPs) Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We will use PyTorch for building and training neural networks, NumPy for numerical operations, and Matplotlib for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch library, which is the main library we'll be using for deep learning\n",
    "import torch\n",
    "\n",
    "# Import the neural network module from PyTorch, which provides building blocks for creating neural networks\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the optimization module from PyTorch, which contains various optimization algorithms\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import the functional module from PyTorch, which contains various functions for neural networks\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import the NumPy library for numerical operations (we might use this for data manipulation)\n",
    "import numpy as np\n",
    "\n",
    "# Import the Matplotlib library for plotting (we'll use this to visualize our results)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for PyTorch to ensure reproducibility\n",
    "# This means that random operations will give the same results each time we run the code\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set a random seed for NumPy to ensure reproducibility\n",
    "# This is similar to the PyTorch seed, but for NumPy operations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA (GPU support) or Apple sillicon is available and set the device accordingly\n",
    "# This line creates a 'device' object that we'll use to tell PyTorch where to run our computations\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print the device being used (CPU or GPU)\n",
    "# This helps us confirm whether we're using GPU acceleration or not\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PyTorch and Tensors\n",
    "\n",
    "PyTorch is a powerful library for machine learning, especially deep learning. It's like a toolbox full of useful tools for building and training neural networks. PyTorch is widely used in both academia and industry due to its flexibility and ease of use.\n",
    "\n",
    "The most basic building block in PyTorch is called a **tensor**. You can think of a tensor as a container for numbers, similar to a list or an array in other programming languages. The cool thing about tensors is that they can have multiple dimensions, making them perfect for representing complex data like images or text. Tensors are also optimized for GPU acceleration, which makes them very efficient for large-scale computations.\n",
    "\n",
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch library, which we'll use to create and manipulate tensors\n",
    "import torch\n",
    "\n",
    "# Create a 1D tensor (vector) with values [1, 2, 3, 4, 5]\n",
    "# A 1D tensor is like a list of numbers\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Create a 2D tensor (matrix) with values [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "# A 2D tensor is like a table of numbers with rows and columns\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Create a 3D tensor with values [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n",
    "# A 3D tensor is like a cube of numbers, or a stack of 2D matrices\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# We can print these tensors to see their contents:\n",
    "print(\"1D tensor (vector):\", vector)\n",
    "print(\"2D tensor (matrix):\", matrix)\n",
    "print(\"3D tensor:\", tensor_3d)\n",
    "\n",
    "# We can also check the shape (dimensions) of each tensor:\n",
    "print(\"Vector shape:\", vector.shape)\n",
    "print(\"Matrix shape:\", matrix.shape)\n",
    "print(\"3D tensor shape:\", tensor_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create tensors of different dimensions. The 1D tensor is like a list, the 2D tensor is like a table or matrix, and the 3D tensor is like a stack of 2D matrices. Understanding these different dimensions is crucial because neural networks often work with high-dimensional data, such as images (which can be represented as 3D tensors) or batches of data (which can be represented as 4D tensors).\n",
    "\n",
    "> **Intuition:** Think of tensors as flexible containers that can hold numbers in various shapes. A 1D tensor is like a line of numbers, a 2D tensor is like a grid of numbers, and a 3D tensor is like a cube of numbers.\n",
    "\n",
    "> **Common Mistake:** Beginners often forget that PyTorch uses zero-based indexing. This means the first element of a tensor is accessed with index 0, not 1.\n",
    "\n",
    "## 2. Tensor Operations and Matrix Math\n",
    "\n",
    "Tensors aren't just for storing numbers; we can perform various operations on them. These operations are the building blocks of neural network computations.\n",
    "\n",
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D tensor 'a' with values [1, 2, 3]\n",
    "a = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create another 1D tensor 'b' with values [4, 5, 6]\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Perform element-wise addition of tensors a and b\n",
    "# This adds corresponding elements: [1+4, 2+5, 3+6]\n",
    "c = a + b  # Result: tensor([5, 7, 9])\n",
    "print(\"Element-wise addition:\", c)\n",
    "\n",
    "# Perform element-wise multiplication of tensors a and b\n",
    "# This multiplies corresponding elements: [1*4, 2*5, 3*6]\n",
    "d = a * b  # Result: tensor([4, 10, 18])\n",
    "print(\"Element-wise multiplication:\", d)\n",
    "\n",
    "# Create a 2D tensor (matrix) 'm1' with values [[1, 2], [3, 4]]\n",
    "m1 = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Create another 2D tensor (matrix) 'm2' with values [[5, 6], [7, 8]]\n",
    "m2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Perform matrix multiplication of tensors m1 and m2\n",
    "# This is different from element-wise multiplication!\n",
    "m3 = torch.matmul(m1, m2)  # Result: tensor([[19, 22], [43, 50]])\n",
    "print(\"Matrix multiplication:\", m3)\n",
    "\n",
    "# Let's break down the matrix multiplication:\n",
    "# [1 2] * [5 6] = 1*5 + 2*7 = 19  |  1*6 + 2*8 = 22\n",
    "# [3 4]   [7 8]   3*5 + 4*7 = 43  |  3*6 + 4*8 = 50\n",
    "\n",
    "# We can also use the '@' operator for matrix multiplication:\n",
    "m4 = m1 @ m2\n",
    "print(\"Matrix multiplication using @:\", m4)\n",
    "\n",
    "# Verify that m3 and m4 are the same\n",
    "print(\"Are m3 and m4 equal?\", torch.all(m3 == m4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations allow us to manipulate tensors in various ways, which is crucial for building and training neural networks. For example, matrix multiplication is a fundamental operation in neural networks, as it is used in the forward pass to compute the activations of neurons. Understanding these operations will help you implement and debug neural network models more effectively.\n",
    "\n",
    "> **Intuition:** Think of tensor operations as ways to combine or transform the numbers inside tensors. Addition is like combining two sets of numbers, while matrix multiplication is a more complex way of combining numbers that's especially useful in neural networks.\n",
    "\n",
    "> **Common Mistake:** Beginners often confuse element-wise multiplication (*) with matrix multiplication (torch.matmul or @). Make sure you use the right operation for your task!\n",
    "\n",
    "## 3. Building Blocks of Neural Networks\n",
    "\n",
    "Neural networks are made up of several key components. Let's explore each one:\n",
    "\n",
    "### Linear Layers\n",
    "\n",
    "A linear layer performs a linear transformation on the input data. It's like applying a mathematical function y = mx + b to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neural network module from PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer that takes 10 input features and produces 5 output features\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# A linear layer performs the operation: y = xW^T + b\n",
    "# Where x is the input, W is the weight matrix, and b is the bias vector\n",
    "\n",
    "# We can inspect the parameters of our linear layer:\n",
    "print(\"Weight matrix shape:\", linear.weight.shape)  # Should be (5, 10)\n",
    "print(\"Bias vector shape:\", linear.bias.shape)      # Should be (5,)\n",
    "\n",
    "# Let's create a sample input and pass it through our linear layer\n",
    "sample_input = torch.randn(3, 10)  # 3 samples, each with 10 features\n",
    "output = linear(sample_input)\n",
    "\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "print(\"Output shape:\", output.shape)  # Should be (3, 5)\n",
    "\n",
    "# The output has 5 features for each of our 3 input samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a linear layer that takes 10 input features and produces 5 output features. Linear layers are the building blocks of neural networks. They apply a linear transformation to the input data, which is essentially a weighted sum of the input features. This transformation allows the network to learn relationships between the input features and the target output.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU (Rectified Linear Unit) activation function\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Create a Sigmoid activation function\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Create a Tanh (Hyperbolic Tangent) activation function\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "# Let's create a sample input to test these activation functions\n",
    "sample_input = torch.tensor([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Apply ReLU activation\n",
    "relu_output = relu(sample_input)\n",
    "print(\"ReLU output:\", relu_output)\n",
    "# ReLU replaces negative values with 0, and keeps positive values as they are\n",
    "\n",
    "# Apply Sigmoid activation\n",
    "sigmoid_output = sigmoid(sample_input)\n",
    "print(\"Sigmoid output:\", sigmoid_output)\n",
    "# Sigmoid squashes values to the range (0, 1)\n",
    "\n",
    "# Apply Tanh activation\n",
    "tanh_output = tanh(sample_input)\n",
    "print(\"Tanh output:\", tanh_output)\n",
    "# Tanh squashes values to the range (-1, 1)\n",
    "\n",
    "# Let's visualize these activation functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title('ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('Sigmoid')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(x, tanh(x))\n",
    "plt.title('Tanh')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are different types of activation functions, each with its own characteristics. Activation functions introduce non-linearity into the network, which allows it to learn complex patterns in the data. Without activation functions, the network would only be able to learn linear relationships, which are not sufficient for most real-world tasks.\n",
    "\n",
    "- ReLU (Rectified Linear Unit): Returns 0 for negative inputs, and the input itself for positive inputs.\n",
    "- Sigmoid: Squashes inputs to a range between 0 and 1.\n",
    "- Tanh: Squashes inputs to a range between -1 and 1.\n",
    "\n",
    "> **Intuition:** Think of activation functions as decision-makers. They look at the input and decide how much of that information should be passed on to the next layer.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique that helps prevent overfitting by randomly \"dropping out\" (setting to zero) a proportion of neurons during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dropout layer with a dropout probability of 0.5 (50%)\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# Dropout is a regularization technique to prevent overfitting\n",
    "# During training, it randomly sets a fraction of input units to 0 at each update\n",
    "\n",
    "# Let's create a sample input to demonstrate dropout\n",
    "sample_input = torch.ones(10, 10)  # 10x10 tensor of ones\n",
    "\n",
    "# Apply dropout during training\n",
    "dropout.train()  # Set the layer to training mode\n",
    "train_output = dropout(sample_input)\n",
    "print(\"Output during training (with dropout):\")\n",
    "print(train_output)\n",
    "\n",
    "# Notice that approximately half of the elements are zero\n",
    "\n",
    "# During evaluation, dropout behaves differently\n",
    "dropout.eval()  # Set the layer to evaluation mode\n",
    "eval_output = dropout(sample_input)\n",
    "print(\"\\nOutput during evaluation (without dropout):\")\n",
    "print(eval_output)\n",
    "\n",
    "# During evaluation, all elements remain unchanged\n",
    "\n",
    "# It's important to set your model to train() mode during training\n",
    "# and eval() mode during evaluation to ensure proper behavior of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a dropout layer that will randomly zero out 50% of the inputs during training. Dropout is a regularization technique that helps prevent overfitting by ensuring that the network does not rely too heavily on any single neuron. By randomly dropping out neurons during training, dropout forces the network to learn more robust features that generalize better to new data.\n",
    "\n",
    "> **Common Question:** \"Why do we need dropout?\" Dropout helps the network learn more robust features by preventing it from relying too heavily on any particular neuron.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes the inputs to a layer for each mini-batch, which can help the network train faster and more stably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batch Normalization layer for inputs with 10 features\n",
    "batch_norm = nn.BatchNorm1d(10)\n",
    "\n",
    "# Batch Normalization normalizes the input to have zero mean and unit variance\n",
    "# It helps in faster convergence and allows higher learning rates\n",
    "\n",
    "# Let's create a sample batch of data to demonstrate batch normalization\n",
    "sample_batch = torch.randn(32, 10)  # 32 samples, each with 10 features\n",
    "\n",
    "# Apply batch normalization\n",
    "normalized_batch = batch_norm(sample_batch)\n",
    "\n",
    "print(\"Original batch mean:\", sample_batch.mean(dim=0))\n",
    "print(\"Original batch std:\", sample_batch.std(dim=0))\n",
    "\n",
    "print(\"\\nNormalized batch mean:\", normalized_batch.mean(dim=0))\n",
    "print(\"Normalized batch std:\", normalized_batch.std(dim=0))\n",
    "\n",
    "# Notice that the normalized batch has mean close to 0 and std close to 1 for each feature\n",
    "\n",
    "# Batch Normalization also has learnable parameters: weight and bias\n",
    "print(\"\\nBatch Norm weight:\", batch_norm.weight)\n",
    "print(\"Batch Norm bias:\", batch_norm.bias)\n",
    "\n",
    "# These parameters allow the layer to learn the optimal scale and shift for each feature\n",
    "\n",
    "# Like dropout, batch normalization behaves differently during training and evaluation\n",
    "# Make sure to set your model to train() mode during training and eval() mode during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a batch normalization layer for inputs with 10 features. Batch normalization normalizes the inputs to a layer for each mini-batch, which can help the network train faster and more stably. By normalizing the inputs, batch normalization reduces the internal covariate shift, which is the change in the distribution of layer inputs during training. This allows for higher learning rates and can lead to faster convergence.\n",
    "\n",
    "## 4. Creating a Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Now that we understand the building blocks, let's put them together to create an MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the Multi-Layer Perceptron (MLP) that inherits from nn.Module\n",
    "class MLP(nn.Module):\n",
    "    # Initialize the MLP with input size, hidden layer sizes, and output size\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        # Call the parent class (nn.Module) constructor\n",
    "        super(MLP, self).__init__()\n",
    "        # Create a list to hold the layers of the MLP\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add the input layer with ReLU activation\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add the hidden layers with ReLU activation\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add the output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "    \n",
    "    # Define the forward pass of the MLP\n",
    "    def forward(self, x):\n",
    "        # Pass the input through each layer in the MLP\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the MLP with input size 10, hidden layer sizes [64, 32], and output size 1\n",
    "mlp = MLP(input_size=10, hidden_sizes=[64, 32], output_size=1)\n",
    "\n",
    "# Let's break down what's happening in this MLP:\n",
    "\n",
    "print(\"MLP Structure:\")\n",
    "for i, layer in enumerate(mlp.layers):\n",
    "    print(f\"Layer {i}: {layer}\")\n",
    "\n",
    "# The MLP has the following structure:\n",
    "# 1. Linear layer: 10 -> 64\n",
    "# 2. ReLU activation\n",
    "# 3. Linear layer: 64 -> 32\n",
    "# 4. ReLU activation\n",
    "# 5. Linear layer: 32 -> 1 (output layer)\n",
    "\n",
    "# Let's create a sample input and pass it through our MLP\n",
    "sample_input = torch.randn(5, 10)  # 5 samples, each with 10 features\n",
    "output = mlp(sample_input)\n",
    "\n",
    "print(\"\\nInput shape:\", sample_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# The output has 1 feature for each of our 5 input samples\n",
    "# This MLP could be used for a regression task or binary classification (with a subsequent sigmoid activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines an MLP with an input size of 10, two hidden layers with 64 and 32 neurons respectively, and an output size of 1. The hidden layers use ReLU activation functions to introduce non-linearity, which allows the network to learn complex patterns in the data. The output layer does not use an activation function, as it is typically used for regression tasks or binary classification (with a subsequent sigmoid activation).\n",
    "\n",
    "> **Intuition:** Think of an MLP as a series of transformations. Each layer takes some input, transforms it, and passes it to the next layer. The final layer produces the output.\n",
    "\n",
    "> **Common Mistake:** Beginners often forget to apply activation functions between linear layers. Without these non-linearities, the entire network would just be one big linear transformation!\n",
    "\n",
    "## Generate a Simple Dataset\n",
    "\n",
    "Before training the MLP, we need to generate a simple dataset. This dataset will be used to train and evaluate the model. For this example, we will create a synthetic dataset where the input features are randomly generated, and the target labels are binary values indicating whether the sum of the input features is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input size\n",
    "input_size = 10  # You can adjust this value as needed\n",
    "\n",
    "# Define a function to generate synthetic data\n",
    "def generate_data(num_samples=1000, input_size=input_size):\n",
    "    # Create input features with random values from a normal distribution\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    # Create binary target labels based on whether the sum of input features is greater than zero\n",
    "    y = (X.sum(dim=1) > 0).float().view(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# Generate training data with 1000 samples\n",
    "X_train, y_train = generate_data(num_samples=1000, input_size=input_size)\n",
    "\n",
    "# Generate test data with 200 samples\n",
    "X_test, y_test = generate_data(num_samples=200, input_size=input_size)\n",
    "\n",
    "# Move the training data to the appropriate device (CPU, CUDA, or MPS)\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "# Move the test data to the appropriate device (CPU, CUDA, or MPS)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Let's examine our generated data\n",
    "print(\"Training data:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"\\nTest data:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Let's look at a few samples\n",
    "print(\"\\nFirst 5 samples of X_train:\")\n",
    "print(X_train[:5])\n",
    "print(\"\\nFirst 5 labels of y_train:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "# We can visualize the distribution of our target variable\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(y_train.cpu().numpy(), bins=2)\n",
    "plt.title('Distribution of y_train')\n",
    "plt.subplot(122)\n",
    "plt.hist(y_test.cpu().numpy(), bins=2)\n",
    "plt.title('Distribution of y_test')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# This synthetic dataset creates a binary classification problem\n",
    "# The task is to predict whether the sum of input features is positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `generate_data` creates a dataset with a specified number of samples. Each sample consists of `input_size` features drawn from a standard normal distribution. The target label `y` is 1 if the sum of the input features is greater than zero, and 0 otherwise. This simple rule allows us to create a binary classification task.\n",
    "\n",
    "We generate separate training and test datasets to evaluate the model's performance on unseen data. The training dataset contains 1000 samples, while the test dataset contains 200 samples. We also move the data to the appropriate device (CPU, CUDA, or MPS) for efficient computation.\n",
    "\n",
    "## 5. Training an MLP\n",
    "\n",
    "Training an MLP involves feeding it data, comparing its predictions to the true values, and adjusting its parameters to improve its predictions. This process is called backpropagation. During backpropagation, the network computes the gradients of the loss with respect to its parameters and updates the parameters using an optimization algorithm. This iterative process continues until the network's performance on the training data improves.\n",
    "\n",
    "### Key Components of Training\n",
    "- **Loss Function:** Measures how far off the model's predictions are from the true values.\n",
    "- **Optimizer:** Adjusts the model's parameters to minimize the loss.\n",
    "- **Training Loop:** Repeatedly feeds data through the model, computes the loss, and updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device\n",
    "mlp = mlp.to(device)\n",
    "\n",
    "# Define the loss function (Binary Cross-Entropy with Logits Loss)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer (Adam) with a learning rate of 0.001\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "# Set the number of epochs (iterations over the entire dataset) for training\n",
    "num_epochs = 50\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    mlp.train()\n",
    "    \n",
    "    # Initialize variables to track training loss and accuracy\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    \n",
    "    # Iterate over batches of data from the training loader\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move batch data to the appropriate device\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Zero the gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform a forward pass to compute the model's predictions\n",
    "        outputs = mlp(batch_X)\n",
    "        \n",
    "        # Compute the loss between the predictions and the true labels\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Perform a backward pass to compute the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Compute the number of correct predictions\n",
    "        train_correct += ((outputs > 0) == batch_y).float().sum().item()\n",
    "    \n",
    "    # Compute average training loss and accuracy for this epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / len(X_train)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    mlp.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        test_outputs = mlp(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test).item()\n",
    "        test_accuracy = ((test_outputs > 0) == y_test).float().mean().item()\n",
    "    \n",
    "    # Store the metrics for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # Print the training and test metrics for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print()\n",
    "\n",
    "# After training, let's visualize the learning curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# This visualization helps us understand how our model's performance improved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we use the Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss) as the loss function, which is suitable for binary classification tasks. The Adam optimizer is used to update the model's parameters. During each epoch, we perform a forward pass to compute the model's predictions, a backward pass to compute the gradients, and an optimization step to update the parameters. We also evaluate the model on the test set every 10 epochs to monitor its performance.\n",
    "\n",
    "> **Intuition:** Think of training as teaching the network. You show it examples (data), see how well it does (loss), and then give it feedback on how to improve (backpropagation).\n",
    "\n",
    "> **Common Mistake:** Forgetting to zero out the gradients (optimizer.zero_grad()) before the backward pass. This can lead to incorrect gradient calculations and poor training.\n",
    "\n",
    "## 6. Evaluating and Using the Trained MLP\n",
    "\n",
    "After training, we need to evaluate how well our model performs on new, unseen data. This step is crucial because it allows us to assess the model's generalization ability, which is its performance on data it has never seen before. A model that performs well on the training data but poorly on new data is likely overfitting, meaning it has learned to memorize the training data rather than generalize from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "mlp.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "    test_outputs = mlp(X_test)  # Perform a forward pass on the test data\n",
    "    test_loss = criterion(test_outputs, y_test)  # Compute the loss on the test data\n",
    "    accuracy = ((test_outputs > 0) == y_test).float().mean()  # Compute the accuracy on the test data\n",
    "    \n",
    "# Print the final test loss and accuracy\n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = torch.randn(5, input_size).to(device)  # Generate new random data and move to the appropriate device\n",
    "with torch.no_grad():  # Disable gradient calculation for prediction\n",
    "    predictions = mlp(new_data)  # Perform a forward pass on the new data\n",
    "    binary_predictions = (predictions > 0).float()  # Convert raw predictions to binary predictions\n",
    "\n",
    "# Print the predictions on new data\n",
    "print(\"\\nPredictions on new data:\")\n",
    "for i, (pred, binary_pred) in enumerate(zip(predictions, binary_predictions)):\n",
    "    print(f\"Sample {i+1}: Raw prediction = {pred.item():.4f}, Binary prediction = {binary_pred.item():.0f}\")\n",
    "\n",
    "# Let's visualize the decision boundary of our trained model\n",
    "# We'll create a grid of points and see how our model classifies them\n",
    "\n",
    "# Generate a grid of points\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y = torch.linspace(-3, 3, 100)\n",
    "xx, yy = torch.meshgrid(x, y)\n",
    "grid = torch.zeros(10000, 10).to(device)\n",
    "grid[:, 0] = xx.flatten()\n",
    "grid[:, 1] = yy.flatten()\n",
    "\n",
    "# Make predictions on the grid points\n",
    "with torch.no_grad():\n",
    "    z = mlp(grid).view(100, 100).cpu()\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx.cpu(), yy.cpu(), z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "plt.colorbar()\n",
    "plt.title('Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(X_train[:, 0].cpu(), X_train[:, 1].cpu(), c=y_train.cpu(), cmap=plt.cm.RdYlBu, edgecolor='black')\n",
    "plt.show()\n",
    "\n",
    "# This visualization helps us understand how our model is making decisions\n",
    "# The blue region represents where the model predicts 0, and the red region where it predicts 1\n",
    "# The scattered points are our training data, colored according to their true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the evaluation phase, we set the model to evaluation mode using `mlp.eval()` and disable gradient calculation with `torch.no_grad()`. This ensures that the model's parameters are not updated during evaluation. We then compute the test loss and accuracy to assess the model's performance. Finally, we make predictions on new data to see how the model generalizes to unseen inputs.\n",
    "\n",
    "> **Intuition:** Evaluation is like giving your trained model a final exam. You test it on data it hasn't seen before to see how well it has learned to generalize.\n",
    "\n",
    "> **Common Mistake:** Forgetting to set the model to evaluation mode (model.eval()). This can lead to incorrect results, especially if you're using layers like Dropout or BatchNorm.\n",
    "\n",
    "## Advanced Topics and Best Practices\n",
    "\n",
    "Here are some advanced topics and best practices to consider when working with MLPs in PyTorch. These techniques can help you improve the performance and robustness of your models:\n",
    "\n",
    "- **Hyperparameter Tuning:** Use techniques like grid search, random search, or Bayesian optimization to find the best hyperparameters (e.g., learning rate, network architecture, dropout rate).\n",
    "\n",
    "- **Regularization:** Implement L1/L2 regularization to prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate Scheduling:** Implement learning rate decay to improve convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Early Stopping:** Implement early stopping to prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ... training code ...\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Saving and Loading:** Save and load your trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "torch.save(mlp.state_dict(), 'mlp_model.pth')\n",
    "\n",
    "# Loading\n",
    "mlp =MLP(input_size, hidden_sizes, output_size)\n",
    "mlp.load_state_dict(torch.load('mlp_model.pth'))\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Augmentation:** For image data, consider using data augmentation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transfer Learning:** For complex tasks, consider using pre-trained models and fine-tuning them for your specific task.\n",
    "\n",
    "- **Gradient Clipping:** Implement gradient clipping to prevent exploding gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Proper Initialization:** Use appropriate weight initialization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "mlp.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Monitoring and Visualization:** Use tools like TensorBoard or Weight & Biases to monitor and visualize your training process.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We've covered the basics of PyTorch and Multi-Layer Perceptrons, from tensors and basic operations to building, training, and evaluating a neural network. Remember, practice is key to mastering these concepts. Don't be afraid to experiment and make mistakes - that's how you learn! By understanding the underlying principles and applying best practices, you can build powerful and efficient neural networks for a wide range of tasks.\n",
    "\n",
    "Happy coding and machine learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
