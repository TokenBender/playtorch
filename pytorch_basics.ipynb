{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# PyTorch Basics and Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "Welcome to this comprehensive tutorial on PyTorch basics and Multi-Layer Perceptrons (MLPs)! In this notebook, we will cover fundamental concepts of PyTorch, including tensor operations, neural network creation, and training. We'll also dive deep into the world of MLPs, exploring their architecture, implementation, and training process using PyTorch.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to PyTorch and Tensors](#introduction-to-pytorch-and-tensors)\n",
    "2. [Tensor Operations and Matrix Math](#tensor-operations-and-matrix-math)\n",
    "3. [Building Blocks of Neural Networks](#building-blocks-of-neural-networks)\n",
    "4. [Creating a Multi-Layer Perceptron (MLP)](#creating-a-multi-layer-perceptron-mlp)\n",
    "5. [Training an MLP](#training-an-mlp)\n",
    "6. [Evaluating and Using the Trained MLP](#evaluating-and-using-the-trained-mlp)\n",
    "7. [Advanced Topics and Best Practices](#advanced-topics-and-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b46e7-9d91-4f16-bcf7-04288ed2b70d",
   "metadata": {},
   "source": [
    "##### If CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f886ff-bcfb-4a8c-b653-e35ff145ab13",
   "metadata": {},
   "source": [
    "##### If Apple sillicon device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015fab3a-0551-4231-8a62-324de1033dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction-to-pytorch-and-tensors",
   "metadata": {},
   "source": [
    "## 1. Introduction to PyTorch and Tensors\n",
    "\n",
    "PyTorch is a powerful deep learning framework that provides a flexible and intuitive way to build and train neural networks. At its core, PyTorch uses tensors, which are multi-dimensional arrays similar to NumPy arrays but with additional features for GPU acceleration and automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors\n",
    "scalar = torch.tensor(42)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"Scalar:\", scalar)\n",
    "print(\"Vector:\", vector)\n",
    "print(\"Matrix:\", matrix)\n",
    "print(\"3D Tensor:\", tensor_3d)\n",
    "\n",
    "# Tensor properties\n",
    "print(\"\\nTensor properties:\")\n",
    "print(\"Shape:\", tensor_3d.shape)\n",
    "print(\"Datatype:\", tensor_3d.dtype)\n",
    "print(\"Device:\", tensor_3d.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor-operations",
   "metadata": {},
   "source": [
    "## 2. Tensor Operations and Matrix Math\n",
    "\n",
    "Understanding tensor operations is crucial for working with neural networks. These operations form the basis of the computations performed in MLPs and other neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor-operations-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic arithmetic operations\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "print(\"Addition:\", a + b)\n",
    "print(\"Subtraction:\", a - b)\n",
    "print(\"Element-wise multiplication:\", a * b)\n",
    "print(\"Element-wise division:\", a / b)\n",
    "\n",
    "# Matrix multiplication\n",
    "m1 = torch.tensor([[1, 2], [3, 4]])\n",
    "m2 = torch.tensor([[5, 6], [7, 8]])\n",
    "print(\"\\nMatrix multiplication:\")\n",
    "print(torch.matmul(m1, m2))\n",
    "\n",
    "# Reshaping tensors\n",
    "original = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "reshaped = original.view(2, 3)\n",
    "print(\"\\nOriginal tensor:\", original)\n",
    "print(\"Reshaped tensor:\")\n",
    "print(reshaped)\n",
    "\n",
    "# Transposing tensors\n",
    "print(\"\\nTransposed tensor:\")\n",
    "print(reshaped.t())\n",
    "\n",
    "# Aggregation operations\n",
    "print(\"\\nMean:\", reshaped.mean())\n",
    "print(\"Sum:\", reshaped.sum())\n",
    "print(\"Max:\", reshaped.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "building-blocks",
   "metadata": {},
   "source": [
    "## 3. Building Blocks of Neural Networks\n",
    "\n",
    "Before we dive into creating an MLP, let's explore some of the fundamental building blocks of neural networks in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "building-blocks-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "input_tensor = torch.randn(3, 10)  # Batch of 3 samples, each with 10 features\n",
    "output = linear(input_tensor)\n",
    "print(\"Linear layer output shape:\", output.shape)\n",
    "\n",
    "# Activation functions\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "print(\"\\nActivation functions:\")\n",
    "print(\"ReLU:\", relu(torch.tensor([-1, 0, 1])))\n",
    "print(\"Sigmoid:\", sigmoid(torch.tensor([-1, 0, 1])))\n",
    "print(\"Tanh:\", tanh(torch.tensor([-1, 0, 1])))\n",
    "\n",
    "# Dropout\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "input_tensor = torch.ones(10)\n",
    "print(\"\\nDropout (training mode):\")\n",
    "print(dropout(input_tensor))\n",
    "\n",
    "# BatchNorm\n",
    "batch_norm = nn.BatchNorm1d(10)\n",
    "input_tensor = torch.randn(20, 10)  # 20 samples, 10 features each\n",
    "normalized = batch_norm(input_tensor)\n",
    "print(\"\\nBatchNorm output mean:\", normalized.mean(dim=0))\n",
    "print(\"BatchNorm output std:\", normalized.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creating-mlp",
   "metadata": {},
   "source": [
    "## 4. Creating a Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Now that we understand the building blocks, let's create a Multi-Layer Perceptron (MLP) using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlp-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "        self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the MLP\n",
    "input_size = 10\n",
    "hidden_sizes = [64, 32, 16]\n",
    "output_size = 1\n",
    "mlp = MLP(input_size, hidden_sizes, output_size)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-mlp",
   "metadata": {},
   "source": [
    "## 5. Training an MLP\n",
    "\n",
    "Now that we have defined our MLP, let's train it on a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple dataset\n",
    "def generate_data(num_samples=1000):\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    y = (X.sum(dim=1) > 0).float().view(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate_data()\n",
    "X_test, y_test = generate_data(200)\n",
    "\n",
    "# Move data to the appropriate device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "mlp = mlp.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = mlp(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            accuracy = ((test_outputs > 0) == y_test).float().mean()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluating-mlp",
   "metadata": {},
   "source": [
    "## 6. Evaluating and Using the Trained MLP\n",
    "\n",
    "Now that we have trained our MLP, let's evaluate its performance and use it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-mlp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = mlp(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    accuracy = ((test_outputs > 0) == y_test).float().mean()\n",
    "    \n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = torch.randn(5, input_size).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = mlp(new_data)\n",
    "    binary_predictions = (predictions > 0).float()\n",
    "\n",
    "print(\"\\nPredictions on new data:\")\n",
    "for i, (pred, binary_pred) in enumerate(zip(predictions, binary_predictions)):\n",
    "    print(f\"Sample {i+1}: Raw prediction = {pred.item():.4f}, Binary prediction = {binary_pred.item():.0f}\")\n",
    "\n",
    "# Visualize decision boundary (for 2D input)\n",
    "if input_size == 2:\n",
    "    x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "    y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100), torch.linspace(y_min, y_max, 100))\n",
    "    Z = mlp(torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1).to(device))\n",
    "    Z = Z.reshape(xx.shape).detach().cpu()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "    plt.scatter(X_test[:, 0].cpu(), X_test[:, 1].cpu(), c=y_test.cpu(), cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('MLP Decision Boundary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-topics",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics and Best Practices\n",
    "\n",
    "Here are some advanced topics and best practices to consider when working with MLPs in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-topics-content",
   "metadata": {},
   "source": [
    "1. **Hyperparameter Tuning**: Use techniques like grid search, random search, or Bayesian optimization to find the best hyperparameters (e.g., learning rate, network architecture, dropout rate).\n",
    "\n",
    "2. **Regularization**: Implement L1/L2 regularization to prevent overfitting:\n",
    "   ```python\n",
    "   optimizer = optim.Adam(mlp.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization\n",
    "   ```\n",
    "\n",
    "3. **Learning Rate Scheduling**: Implement learning rate decay to improve convergence:\n",
    "   ```python\n",
    "   scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "   ```\n",
    "\n",
    "4. **Early Stopping**: Implement early stopping to prevent overfitting:\n",
    "   ```python\n",
    "   best_loss = float('inf')\n",
    "   patience = 10\n",
    "   counter = 0\n",
    "   \n",
    "   for epoch in range(num_epochs):\n",
    "       # ... training code ...\n",
    "       \n",
    "       if val_loss < best_loss:\n",
    "           best_loss = val_loss\n",
    "           counter = 0\n",
    "       else:\n",
    "           counter += 1\n",
    "           if counter >= patience:\n",
    "               print(f\"Early stopping at epoch {epoch}\")\n",
    "               break\n",
    "   ```\n",
    "\n",
    "5. **Model Saving and Loading**: Save and load your trained models:\n",
    "   ```python\n",
    "   # Saving\n",
    "   torch.save(mlp.state_dict(), 'mlp_model.pth')\n",
    "   \n",
    "   # Loading\n",
    "   mlp = MLP(input_size, hidden_sizes, output_size)\n",
    "   mlp.load_state_dict(torch.load('mlp_model.pth'))\n",
    "   mlp.eval()\n",
    "   ```\n",
    "\n",
    "6. **Data Augmentation**: For image data, consider using data augmentation techniques:\n",
    "   ```python\n",
    "   transform = transforms.Compose([\n",
    "       transforms.RandomHorizontalFlip(),\n",
    "       transforms.RandomRotation(10),\n",
    "       transforms.ToTensor(),\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "7. **Transfer Learning**: For complex tasks, consider using pre-trained models and fine-tuning them for your specific task.\n",
    "\n",
    "8. **Gradient Clipping**: Implement gradient clipping to prevent exploding gradients:\n",
    "   ```python\n",
    "   torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=1.0)\n",
    "   ```\n",
    "\n",
    "9. **Proper Initialization**: Use appropriate weight initialization techniques:\n",
    "   ```python\n",
    "   def init_weights(m):\n",
    "       if type(m) == nn.Linear:\n",
    "           torch.nn.init.xavier_uniform_(m.weight)\n",
    "           m.bias.data.fill_(0.01)\n",
    "   \n",
    "   mlp.apply(init_weights)\n",
    "   ```\n",
    "\n",
    "10. **Monitoring and Visualization**: Use tools like TensorBoard or Weight & Biases to monitor and visualize your training process.\n",
    "\n",
    "By implementing these advanced techniques and best practices, you can significantly improve the performance and robustness of your MLPs in PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
